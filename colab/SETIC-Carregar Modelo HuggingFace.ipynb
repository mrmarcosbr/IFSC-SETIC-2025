{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Q557aQA3elg2koAU9oOotcyEpjBaiee-",
     "timestamp": 1749610771161
    },
    {
     "file_id": "1fAT24UHzsk6U0yKGe0YpMHuhQ4yrrYuG",
     "timestamp": 1749342840673
    },
    {
     "file_id": "16j93cSJFi9uGQN7fgimndk4D49fQTYww",
     "timestamp": 1714049288732
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Demonstração de Uso de Ferramentas para Manipulação de Modelos de Linguagem\n",
    "\n",
    "Nesta seção, aplicamos os conceitos apresentados nas seções anteriores em uma demonstração prática. Vamos explorar o uso de um modelo de linguagem generativa pré-treinado, desde a inicialização até a geração de texto, com exemplos claros e objetivos.\n",
    "\n",
    "**Objetivo da demonstração:**\n",
    "* Mostrar como carregar e usar um modelo pré-treinado da biblioteca Hugging Face.\n",
    "* Gerar texto a partir de prompts personalizados.\n",
    "* Explorar como diferentes configurações de parâmetros impactam os resultados.\n",
    "* Ao final, você será capaz de utilizar modelos de linguagem para tarefas práticas como geração de texto criativo, resumos ou respostas para perguntas específicas.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "OJA5knmZwYFH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Instalar a biblioteca Transformers (caso não esteja instalada)"
   ],
   "metadata": {
    "id": "H6744WfxoP7S"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "id": "gOK_tAsToR3A",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1749340577551,
     "user_tz": 180,
     "elapsed": 7586,
     "user": {
      "displayName": "Marcos Heyse",
      "userId": "10081780646752173300"
     }
    },
    "outputId": "d96ef4fe-2954-44bf-c88d-c9806c115de0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-06-18T01:46:44.024312Z",
     "start_time": "2025-06-18T01:44:47.134190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/96/f2/25b27b396af03d5b64e61976b14f7209e2939e9e806c10749b6d277c273e/transformers-4.52.4-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/4d/36/2a115987e2d8c300a974597416d9de88f2444426de9571f4b59b2cca3acc/filelock-3.18.0-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.30.0 from https://files.pythonhosted.org/packages/33/fb/53587a89fbc00799e4179796f51b3ad713c5de6bb680b2becb6d37c94649/huggingface_hub-0.33.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Obtaining dependency information for numpy>=1.17 from https://files.pythonhosted.org/packages/08/60/61d60cf0dfc0bf15381eaef46366ebc0c1a787856d1db0c80b006092af84/numpy-2.3.0-cp313-cp313-win_amd64.whl.metadata\n",
      "  Downloading numpy-2.3.0-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     -------------------------------- ------ 51.2/60.9 kB 64.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 51.2/60.9 kB 64.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 51.2/60.9 kB 64.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 51.2/60.9 kB 64.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 60.9/60.9 kB 56.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\desenvolvimento\\python\\ifsc-setic-2025\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\desenvolvimento\\python\\ifsc-setic-2025\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/45/94/bc295babb3062a731f52621cdc992d123111282e291abaf23faa413443ea/regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.5/41.5 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in e:\\desenvolvimento\\python\\ifsc-setic-2025\\.venv\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/e6/b6/072a8e053ae600dcc2ac0da81a23548e3b523301a442a6ca900e92ac35be/tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.3 from https://files.pythonhosted.org/packages/69/e2/b011c38e5394c4c18fb5500778a55ec43ad6106126e74723ffaee246f56e/safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Obtaining dependency information for tqdm>=4.27 from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.7/57.7 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/bb/61/78c7b3851add1481b048b5fdc29067397a1784e2910592bc81bb3f608635/fsspec-2025.5.1-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\desenvolvimento\\python\\ifsc-setic-2025\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: colorama in e:\\desenvolvimento\\python\\ifsc-setic-2025\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\desenvolvimento\\python\\ifsc-setic-2025\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\desenvolvimento\\python\\ifsc-setic-2025\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\desenvolvimento\\python\\ifsc-setic-2025\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\desenvolvimento\\python\\ifsc-setic-2025\\.venv\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/10.5 MB 4.5 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.9/10.5 MB 4.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.2/10.5 MB 4.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.5/10.5 MB 4.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.0/10.5 MB 4.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.2/10.5 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.7/10.5 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.3/10.5 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.5/10.5 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.1/10.5 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.4/10.5 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 5.9/10.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.1/10.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.7/10.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.5/10.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.8/10.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.3/10.5 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.5/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.9/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.1/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.3/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.5/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.7/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.8/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.9/10.5 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.1/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.3/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.4/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "   ---------------------------------------- 0.0/514.8 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 174.1/514.8 kB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 368.6/514.8 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  512.0/514.8 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 514.8/514.8 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading numpy-2.3.0-cp313-cp313-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.2/12.7 MB 4.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/12.7 MB 4.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/12.7 MB 3.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.5/12.7 MB 3.8 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.1/12.7 MB 3.6 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.2/12.7 MB 3.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.3/12.7 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.5/12.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.6/12.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.8/12.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.9/12.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.2/12.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.3/12.7 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.5/12.7 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.6/12.7 MB 3.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.7/12.7 MB 3.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.8/12.7 MB 3.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.9/12.7 MB 3.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.0/12.7 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 5.2/12.7 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 5.3/12.7 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 5.8/12.7 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 5.9/12.7 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 6.0/12.7 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 6.2/12.7 MB 3.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 6.6/12.7 MB 2.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.9/12.7 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 7.0/12.7 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 7.3/12.7 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.4/12.7 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.6/12.7 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.7/12.7 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.9/12.7 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 8.1/12.7 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 8.2/12.7 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.9/12.7 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.4/12.7 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.1/12.7 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.2/12.7 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.6/12.7 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.7 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.9/12.7 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.7 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.2/12.7 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.4/12.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.5/12.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.7/12.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.7/12.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "   ---------------------------------------- 0.0/273.5 kB ? eta -:--:--\n",
      "   ------------------------------------- - 266.2/273.5 kB 90.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/273.5 kB 90.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 273.5/273.5 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "   ---------------------------------------- 0.0/308.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 308.9/308.9 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.4/2.4 MB 12.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.4 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.4 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.2/2.4 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.4 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.7/2.4 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.0/2.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.5/78.5 kB 12.8 MB/s eta 0:00:00\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.1 kB ? eta -:--:--\n",
      "   -------------------------------------  194.6/199.1 kB 125.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 199.1/199.1 kB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, safetensors, regex, numpy, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.5.1 huggingface-hub-0.33.0 numpy-2.3.0 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.52.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Carregando o Modelo"
   ],
   "metadata": {
    "id": "eWLdDXO4oTdw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Importa as classes para trabalhar com modelos e tokenizadores\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define o nome de um modelo treinado para português\n",
    "# model_name = \"pierreguillou/gpt2-small-portuguese\"  # Modelo GPT-2 adaptado para Português/BR\n",
    "model_name = \"CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it\"    # Modelo em Português/BR - Google + CEIA/UFG\n",
    "\n",
    "# Carrega o modelo e o tokenizador correspondentes\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ],
   "metadata": {
    "id": "i_I6CS9doVrI",
    "ExecuteTime": {
     "end_time": "2025-06-18T01:47:36.967977Z",
     "start_time": "2025-06-18T01:47:24.765967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97e0acff457346bf84205b1678240506"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Desenvolvimento\\Python\\IFSC-SETIC-2025\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\marco\\.cache\\huggingface\\hub\\models--CEIA-UFG--Gemma-3-Gaia-PT-BR-4b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a4a46f37c5d4a9da0f8a3c9aea9319d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af9be5eea06f401fbc0e2768c8455fd1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "893fa099492b4c68b986b239db972a9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "141face309794c6e9866f7e8dada9db2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# Carrega o modelo e o tokenizador correspondentes\u001B[39;00m\n\u001B[32m      9\u001B[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m model = \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m(model_name)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Desenvolvimento\\Python\\IFSC-SETIC-2025\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1885\u001B[39m, in \u001B[36mDummyObject.__getattribute__\u001B[39m\u001B[34m(cls, key)\u001B[39m\n\u001B[32m   1883\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (key.startswith(\u001B[33m\"\u001B[39m\u001B[33m_\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m key != \u001B[33m\"\u001B[39m\u001B[33m_from_config\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m key == \u001B[33m\"\u001B[39m\u001B[33mis_dummy\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m key == \u001B[33m\"\u001B[39m\u001B[33mmro\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m key == \u001B[33m\"\u001B[39m\u001B[33mcall\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1884\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__getattribute__\u001B[39m(key)\n\u001B[32m-> \u001B[39m\u001B[32m1885\u001B[39m \u001B[43mrequires_backends\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_backends\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Desenvolvimento\\Python\\IFSC-SETIC-2025\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1871\u001B[39m, in \u001B[36mrequires_backends\u001B[39m\u001B[34m(obj, backends)\u001B[39m\n\u001B[32m   1868\u001B[39m         failed.append(msg.format(name))\n\u001B[32m   1870\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m failed:\n\u001B[32m-> \u001B[39m\u001B[32m1871\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m.join(failed))\n",
      "\u001B[31mImportError\u001B[39m: \nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gerando Texto a Partir de um Prompt"
   ],
   "metadata": {
    "id": "wONeYb80oYD5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define o prompt inicial que será usado como entrada para o modelo\n",
    "prompt = \"\"\"\n",
    "Escreva um parágrafo em português sobre como a inteligência artificial está transformando o setor de saúde.\n",
    "Foque em exemplos como diagnósticos precisos, análise de dados médicos e personalização de tratamentos.\n",
    "\"\"\"\n",
    "\n",
    "# Tokeniza o prompt, convertendo-o em tensores que o modelo pode processar\n",
    "# - return_tensors=\"pt\": Retorna os tokens no formato de tensor PyTorch\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Gera texto com base no prompt tokenizado\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_length=200,  # Limita o comprimento da saída\n",
    "    num_return_sequences=1,  # Gera uma sequência\n",
    "    temperature=0.9,  # Controla a criatividade do texto\n",
    "    do_sample=True,  # Ativa amostragem aleatória\n",
    "    pad_token_id=tokenizer.eos_token_id,  # Define o token de preenchimento\n",
    "    top_p=0.95,\n",
    "    eos_token_id=None\n",
    ")\n",
    "\n",
    "# Decodifica os tokens gerados de volta para texto legível\n",
    "# - skip_special_tokens=True: Remove tokens especiais, como \"<|endoftext|>\"\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Exibe o texto gerado\n",
    "print(generated_text)"
   ],
   "metadata": {
    "id": "ldgCnkPgobUm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1749340618510,
     "user_tz": 180,
     "elapsed": 30803,
     "user": {
      "displayName": "Marcos Heyse",
      "userId": "10081780646752173300"
     }
    },
    "outputId": "0e7fb36c-eabf-4ace-b110-e1c6bf5574c6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Escreva um parágrafo em português sobre como a inteligência artificial está transformando o setor de saúde.\n",
      "Foque em exemplos como diagnósticos precisos, análise de dados médicos e personalização de tratamentos.\n",
      "A ciência é um campo multidisciplinar em que diversas especialidades são usadas, o que tem gerado muitos debates. Os principais ramos dessa ciência são: neurociência, neurobiológicas, neurofisiologia, neurofisiologia animal, neurobiologia, genética e ciências biológicas. O termo ciências é derivado do latim \"scramma\", uma palavra alemã que significa \"observação\" ou \"responsabilidade\", por ter uma conotação filosófica.\n",
      "\n",
      "Existem diversas áreas em que a ciência atua, muitas delas, de acordo com a própria ciência. As primeiras incluem a medicina, a neurologia, a etnografia e o neuropsicologia, mas as áreas atuais incluem a genética, a genética biológica, a neurocirurgia, a biofísica, a neurologia da informação, a neurociência cognitiva, a neurociência\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt para gerar um resumo"
   ],
   "metadata": {
    "id": "hBnxZhwJodVe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt para gerar um resumo\n",
    "prompt = \"Resuma o seguinte texto em poucas palavras: A reciclagem é importante porque reduz resíduos, economiza energia e protege o meio ambiente.\"\n",
    "\n",
    "# inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# outputs = model.generate(inputs, max_length=60, num_return_sequences=1, temperature=0.7)\n",
    "\n",
    "# Define 'input_ids' e 'attention_mask' usando padding para limite/ajuste\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Geração do texto\n",
    "outputs = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n",
    "max_length=60, num_return_sequences=1, temperature=0.7, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "id": "7wDfJhHiofNb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1749341986923,
     "user_tz": 180,
     "elapsed": 2358,
     "user": {
      "displayName": "Marcos Heyse",
      "userId": "10081780646752173300"
     }
    },
    "outputId": "bd6d9b54-91cc-48e5-ca94-5e8d87df699a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Resuma o seguinte texto em poucas palavras: A reciclagem é importante porque reduz resíduos, economiza energia e protege o meio ambiente.\n",
      "\n",
      "Para conseguir reciclagem, é preciso de um bom plano de reciclagem, com boa ênfase na reciclagem de lixo e reciclagem de resíduos sólidos, que são difíceis de se mover\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "jMWB4qtIog49"
   }
  }
 ]
}
